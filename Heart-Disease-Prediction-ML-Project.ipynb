{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1729066964924}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - **Heart**-**Disease**-**Prediction**\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["**Import** **libraries**\n","\n","Let's first import all the necessary libraries. I'll use **numpy** and **pandas** to start with. For visualization, I will use **pyplot** subpackage of **matplotlib**, use **rcParams** to add styling to the plots and **rainbow** for colors. For implementing Machine Learning models and processing of data, I will use the **sklearn** library."],"metadata":{"id":"6PokJsscTlXN"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import rcParams\n","from matplotlib.cm import rainbow\n","%matplotlib inline\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"0vsXfoNHTt3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For processing the data, I'll import a few libraries. To split the available dataset for testing and training, I'll use the **train_test_split** method. To scale the features, I am using **StandardScaler**."],"metadata":{"id":"qO5qDwUOTxMl"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"y7cvHTj8T0bF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, I'll import all the Machine Learning algorithms I will be using.\n","\n","1. K Neighbors Classifier\n","2. Support Vector Classifier\n","3. Decision Tree Classifier\n","4. Random Forest Classifier"],"metadata":{"id":"EO1Dmw13T4Gs"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"noTtAOOxUDli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EgJDtnvKUFRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_csv('/content/drive/MyDrive/Datasets/Heart Disease data.csv')"],"metadata":{"id":"46zusGSgUEhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.info()"],"metadata":{"id":"3y4hEbYeUV-w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks like the dataset has a total of 303 rows and there are no missing values. There are a total of **13** **features** along with one target value which we wish to find."],"metadata":{"id":"wn4JudxuUcxz"}},{"cell_type":"code","source":["  dataset.describe()"],"metadata":{"id":"xxE7HGGpUcW9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The scale of each feature column is different and quite varied as well. While the maximum for **age** reaches 77, the maximum of **chol** (serum cholestoral) is 564."],"metadata":{"id":"-WziML_-U1ng"}},{"cell_type":"markdown","source":["**Understanding** **the** **data**\n","\n","Now, we can use visualizations to better understand our data and then look at any processing we might want to do."],"metadata":{"id":"GWGkJsFNU6V4"}},{"cell_type":"code","source":["rcParams['figure.figsize'] = 20, 14\n","plt.matshow(dataset.corr())\n","plt.yticks(np.arange(dataset.shape[1]), dataset.columns)\n","plt.xticks(np.arange(dataset.shape[1]), dataset.columns)\n","plt.colorbar()"],"metadata":{"id":"UIqQYv3tVAFH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Taking a look at the correlation matrix above, it's easy to see that a few features have negative correlation with the target value while some have positive. Next, I'll take a look at the histograms for each variable."],"metadata":{"id":"RPi81dM5VCbJ"}},{"cell_type":"code","source":["dataset.hist()"],"metadata":{"id":"A_1eRK4TVDF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Taking a look at the histograms above, I can see that each feature has a different range of distribution. Thus, using scaling before our predictions should be of great use. Also, the categorical features do stand out.\n","\n","It's always a good practice to work with a dataset where the target classes are of approximately equal size. Thus, let's check for the same."],"metadata":{"id":"bN355yzBVHPq"}},{"cell_type":"code","source":["rcParams['figure.figsize'] = 8,6\n","plt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\n","plt.xticks([0, 1])\n","plt.xlabel('Target Classes')\n","plt.ylabel('Count')\n","plt.title('Count of each Target Class')"],"metadata":{"id":"s9bsVpNjVJaw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The two classes are not exactly 50% each but the ratio is good enough to continue without dropping/increasing our data."],"metadata":{"id":"emPakzxmVM3L"}},{"cell_type":"markdown","source":["**Data** **Processing**\n","\n","After exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. First, I'll use the **get_dummies** method to create dummy columns for categorical variables."],"metadata":{"id":"N9g7qdpkVP6P"}},{"cell_type":"code","source":["dataset = pd.get_dummies(dataset, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])"],"metadata":{"id":"y7LWCftLVVB5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data is not ready for our Machine Learning application."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["**Machine** **Learning**\n","\n","I'll now import **train_test_split** to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data."],"metadata":{"id":"VAJ0r2CqVdgC"}},{"cell_type":"code","source":["y = dataset['target']\n","X = dataset.drop(['target'], axis = 1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)"],"metadata":{"id":"_COYdgtMVidG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**K Neighbors Classifier**\n","\n","The classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score."],"metadata":{"id":"HC-5tfXJVoHV"}},{"cell_type":"code","source":["knn_scores = []\n","for k in range(1,21):\n","    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n","    knn_classifier.fit(X_train, y_train)\n","    knn_scores.append(knn_classifier.score(X_test, y_test))"],"metadata":{"id":"bdehSpLiVvmk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I have the scores for different neighbor values in the array **knn_scores**. I'll now plot it and see for which value of K did I get the best scores."],"metadata":{"id":"Eti4prifVzjp"}},{"cell_type":"code","source":["plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\n","for i in range(1,21):\n","    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\n","plt.xticks([i for i in range(1, 21)])\n","plt.xlabel('Number of Neighbors (K)')\n","plt.ylabel('Scores')\n","plt.title('K Neighbors Classifier scores for different K values')"],"metadata":{"id":"PsD_wWexV3js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the plot above, it is clear that the maximum score achieved was 0.87 for the 8 neighbors."],"metadata":{"id":"7t5cX7VaWYEN"}},{"cell_type":"code","source":["print(\"The score for K Neighbors Classifier is {}% with {} nieghbors.\".format(knn_scores[7]*100, 8))"],"metadata":{"id":"bx1MMNotWbYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The score for K Neighbors Classifier is 87.0% with 8 nieghbors."],"metadata":{"id":"Gu1_iKu4Wkp5"}},{"cell_type":"markdown","source":["**Support** **Vector** **Classifier**\n","\n","There are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score.\n","\n"],"metadata":{"id":"14IeJNIFWmvy"}},{"cell_type":"code","source":["svc_scores = []\n","kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n","for i in range(len(kernels)):\n","    svc_classifier = SVC(kernel = kernels[i])\n","    svc_classifier.fit(X_train, y_train)\n","    svc_scores.append(svc_classifier.score(X_test, y_test))"],"metadata":{"id":"w0OZZXuRWsm1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I'll now plot a bar plot of scores for each kernel and see which performed the best."],"metadata":{"id":"O7nXJEUrWv5E"}},{"cell_type":"code","source":["colors = rainbow(np.linspace(0, 1, len(kernels)))\n","plt.bar(kernels, svc_scores, color = colors)\n","for i in range(len(kernels)):\n","    plt.text(i, svc_scores[i], svc_scores[i])\n","plt.xlabel('Kernels')\n","plt.ylabel('Scores')\n","plt.title('Support Vector Classifier scores for different kernels')\n"],"metadata":{"id":"IyPdydpBWxt8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The **linear** kernel performed the best, being slightly better than **rbf** kernel."],"metadata":{"id":"dj7PerkIW1li"}},{"cell_type":"code","source":["print(\"The score for Support Vector Classifier is {}% with {} kernel.\".format(svc_scores[0]*100, 'linear'))"],"metadata":{"id":"uGLnQ7NxW6J6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Decision** **Tree** **Classifier**\n","\n","Here, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of **max_features** and see which returns the best accuracy."],"metadata":{"id":"oiYB8a1LXDjb"}},{"cell_type":"code","source":["dt_scores = []\n","for i in range(1, len(X.columns) + 1):\n","    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n","    dt_classifier.fit(X_train, y_train)\n","    dt_scores.append(dt_classifier.score(X_test, y_test))"],"metadata":{"id":"ISDX4thoXIhS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","I selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases.\n","\n"],"metadata":{"id":"01XKmMbsXKdC"}},{"cell_type":"code","source":["plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\n","for i in range(1, len(X.columns) + 1):\n","    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\n","plt.xticks([i for i in range(1, len(X.columns) + 1)])\n","plt.xlabel('Max features')\n","plt.ylabel('Scores')\n","plt.title('Decision Tree Classifier scores for different number of maximum features')"],"metadata":{"id":"WPBQJbV7XLIm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model achieved the best accuracy at three values of maximum features, **2, 4 and 18.**"],"metadata":{"id":"0_1Bij2pXSnM"}},{"cell_type":"code","source":["print(\"The score for Decision Tree Classifier is {}% with {} maximum features.\".format(dt_scores[17]*100, [2,4,18]))"],"metadata":{"id":"x1RIBUYtXVPM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The score for Decision Tree Classifier is 79.0% with [2, 4, 18] maximum features."],"metadata":{"id":"oPC8_GzbXrAC"}},{"cell_type":"markdown","source":["**Random** **Forest** **Classifier**\n","\n","Now, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect."],"metadata":{"id":"JNldHaRsXu61"}},{"cell_type":"code","source":["rf_scores = []\n","estimators = [10, 100, 200, 500, 1000]\n","for i in estimators:\n","    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n","    rf_classifier.fit(X_train, y_train)\n","    rf_scores.append(rf_classifier.score(X_test, y_test))"],"metadata":{"id":"4itZ5_TrXr8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model is trained and the scores are recorded. Let's plot a bar plot to compare the scores."],"metadata":{"id":"IIbC3VCdX5sh"}},{"cell_type":"code","source":["colors = rainbow(np.linspace(0, 1, len(estimators)))\n","plt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\n","for i in range(len(estimators)):\n","    plt.text(i, rf_scores[i], rf_scores[i])\n","plt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\n","plt.xlabel('Number of estimators')\n","plt.ylabel('Scores')\n","plt.title('Random Forest Classifier scores for different number of estimators')\n"],"metadata":{"id":"taNeuiVqX8Q_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The maximum score is achieved when the total estimators are 100 or 500."],"metadata":{"id":"mEoCmvhQX_SQ"}},{"cell_type":"code","source":["print(\"The score for Random Forest Classifier is {}% with {} estimators.\".format(rf_scores[1]*100, [100, 500]))"],"metadata":{"id":"or59deEdYFMX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Conclusion**\n","\n","In this project, I used Machine Learning to predict whether a person is suffering from a heart disease. After importing the data, I analysed it using plots. Then, I did generated dummy variables for categorical features and scaled other features. I then applied four Machine Learning algorithms, **K Neighbors Classifier, Support Vector **Classifier , **Decision** **Tree** **Classifier** **and** **Random** **Forest** **Classifier**. I varied parameters across each model to improve their scores. In the end, **K Neighbors Classifier** achieved the highest score of **87**% with **8 nearest neighbors.**"],"metadata":{"id":"82N_OniAYKYP"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning  Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}